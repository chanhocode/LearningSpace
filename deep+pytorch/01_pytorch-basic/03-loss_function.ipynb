{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function (손실 함수)\n",
    "> 목적함수(Objective Function), 비용함수(Cost Function)\n",
    "\n",
    ": 인공 신경망은 실제값과 예측값을 통해 계산된 오차값을 최소화해 정확도를 높인다. 이때 각 데이터의 오차를 계산하는데, 이때 손실 함수를 사용한다.\n",
    "\n",
    "- `제곱 오차(SE)`: 실제값과 예측값을 감산한 값에 제곱을 취함, 제곱을 취하기 때문에 오차가 커질수록 데이터마다 오차의 크기를 빠르게 확인할 수 있다.\n",
    "- `오차 제곱합(SSE)`: 제곱 오차를 모두 더한 값, 모든 값을 제곱한 값에 대한 평균으로 오차를 계산한다.\n",
    "- `평균 제곱 오차(MSE)`: 오차 제곱합에서 평균을 취하는 방법, 오차가 0에 가까워질수록 높은 품질, 주로 회귀 분석에 사용, 최대 신호 대 잡음비를 계산할때 사용. 해당 값에 루트를 씌우는 경우에는 평균 제곱근 오차(RMSE)를 계산할 때도 사용된다.\n",
    "- `교차 엔트로피`: 이산형 변수에 사용, 실제값의 확률분포와 예측값의 확률분포 차이를 계산한다.\n",
    "\n",
    "# Optimization (최적화)\n",
    "> 목적 함수의 결과값을 최적화라는 변수를 찾는 알고리즘\n",
    "\n",
    "`기울기(Gradient)가 0에 가까워 질 수록 최적의 가중치를 갖음`\n",
    "\n",
    "## 경사 하강법(Gradient Descent)\n",
    "> 함수의 기울기가 낮은 곳으로 계속 이동시켜 극값이 도달할 때 까지 반복하는 알고리즘이다.\n",
    "\n",
    "## 단순 선형 회귀\n",
    "\n",
    "- Learning Rate(a: alpha)\n",
    "  - 가중치를 갱신할 때 a를 곱해 가중치 결과를 조정\n",
    "\n",
    "- Epoch\n",
    "  - 인공 신경망에서 순전파와 역전파 과정 등의 모델 연산을 정체 데이터세트가 1회 통과하는 것\n",
    "  - 에폭이 너무 적을 경우 과소적합이 너무 많을 경우 과대적합이 발생한다.\n",
    "\n",
    "- Beach\n",
    "  - 가설이나 모델의 가중치를 갱신할 때 사용하는 데이터의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/10000 Weight: 0.864258, Bias: -0.138465 Cost: 1.393250\n",
      "Epoch 2000/10000 Weight: 0.869788, Bias: -0.250826 Cost: 1.380349\n",
      "Epoch 3000/10000 Weight: 0.873230, Bias: -0.320745 Cost: 1.375354\n",
      "Epoch 4000/10000 Weight: 0.875371, Bias: -0.364253 Cost: 1.373419\n",
      "Epoch 5000/10000 Weight: 0.876704, Bias: -0.391328 Cost: 1.372670\n",
      "Epoch 6000/10000 Weight: 0.877533, Bias: -0.408175 Cost: 1.372380\n",
      "Epoch 7000/10000 Weight: 0.878049, Bias: -0.418659 Cost: 1.372267\n",
      "Epoch 8000/10000 Weight: 0.878370, Bias: -0.425183 Cost: 1.372224\n",
      "Epoch 9000/10000 Weight: 0.878570, Bias: -0.429243 Cost: 1.372208\n",
      "Epoch 10000/10000 Weight: 0.878694, Bias: -0.431769 Cost: 1.372200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "# optim 은 최적화 함수가 포함돼 있는 모듈\n",
    "\n",
    "x = torch.FloatTensor([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30]])\n",
    "\n",
    "y = torch.FloatTensor([[0.94], [1.98], [2.88], [3.92], [3.96], [4.55], [5.64], [6.3], [7.44], [9.1], [8.46], [9.5], [10.67], [11.16], [14], [11.83], [14.4], [14.25], [16.2], [16.32], [17.46], [19.8], [18], [21.34], [22], [22.5], [24.57], [26.04], [21.6], [28.8]])\n",
    "\n",
    "# 하이퍼파라미터 초기화\n",
    "weight = torch.zeros(1, requires_grad=True) # zeros는 0의 값을 갖는 텐서를 생성 텐서의 크기는 1로 설정하는데 이 값은 (1,) 또는 (1,1)로 설정해도 동일한 결과를 반환한다. requires_grad=True는 이 텐서에 대한 기울기를 저장할지 여부를 나타낸다. 이 값이 True이면 Autograd 기능이 활성화된다.(자동 미분 기능의 사용여부로 생각해도 된다.)\n",
    "bias = torch.zeros(1, requires_grad=True)\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 최적화 선언\n",
    "optimizer = optim.SGD([weight, bias], lr=learning_rate)\n",
    "# SGD는 확률적 경사 하강법을 의미한다. 모든 데이터에 대해 연산을 진행하지 않고 일부 데이터만 게산하여 빠르게 최적화된 값을 찾는 방식이다. lr은 학습률을 의미한다.\n",
    "\n",
    "for epoch in range(10000):\n",
    "  hypothesis = x * weight + bias\n",
    "  cost = torch.mean((hypothesis - y) ** 2)\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "  if (epoch + 1) % 1000 == 0:\n",
    "    print('Epoch {:4d}/{} Weight: {:.6f}, Bias: {:.6f} Cost: {:.6f}'.format(epoch + 1, 10000, weight.item(), bias.item(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/10000 Cost: 707.459717\n",
      "Epoch 2000/10000 Cost: 707.459717\n",
      "Epoch 3000/10000 Cost: 707.459717\n",
      "Epoch 4000/10000 Cost: 707.459717\n",
      "Epoch 5000/10000 Cost: 707.459717\n",
      "Epoch 6000/10000 Cost: 707.459717\n",
      "Epoch 7000/10000 Cost: 707.459717\n",
      "Epoch 8000/10000 Cost: 707.459717\n",
      "Epoch 9000/10000 Cost: 707.459717\n",
      "Epoch 10000/10000 Cost: 707.459717\n"
     ]
    }
   ],
   "source": [
    "# 신경망 패키지 사용\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "x = torch.FloatTensor([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30]])\n",
    "\n",
    "y = torch.FloatTensor([[0.94], [1.98], [2.88], [3.92], [3.96], [4.55], [5.64], [6.3], [7.44], [9.1], [8.46], [9.5], [10.67], [11.16], [14], [11.83], [14.4], [14.25], [16.2], [16.32], [17.46], [19.8], [18], [21.34], [22], [22.5], [24.57], [26.04], [21.6], [28.8]])\n",
    "\n",
    "model = nn.Linear(1, 1, bias=True)\n",
    "criterion = torch.nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(10000):\n",
    "  output = model(x)\n",
    "  cost = criterion(output, y)\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  if (epoch + 1) % 1000 == 0:\n",
    "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch + 1, 10000, cost.item()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
