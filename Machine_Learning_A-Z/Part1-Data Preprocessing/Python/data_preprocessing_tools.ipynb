{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pandas Libray를 사용하면 쉽게 데이터셋 파일을 로드가 가능하다.\n",
    "- CSV File: read_csv('path)\n",
    "- JSON File: read_json('path')\n",
    "\n",
    "> 추가로 내장 모듈인 os, json을 이용해 로드도 가능하다.\n",
    "```python\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 파일 읽기\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 데이터 출력 (dict 또는 list 형태)\n",
    "print(data)\n",
    "\n",
    "# Pandas DataFrame으로 변환\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "file_path = 'data.csv'\n",
    "\n",
    "# 파일 존재 여부 확인 후 로드\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"파일이 존재하지 않습니다.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../Data.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`특성 변수(Feature Variable)`\n",
    ": 모델의 입력으로 사용되는 독립 변수로, 데이터셋에서 예측이나 분석의 기반이 되는 변수\n",
    "\n",
    "`종속 변수(Target Variable)`\n",
    ": 모델의 출력 또는 예측하려는 목표 변수로, 특성 변수에 의해 영향을 받는 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성변수 & 종속 변수\n",
    "x = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x(10): [['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "y(10): ['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(f\"x({len(x)}): {x}\")\n",
    "print(f\"y({len(y)}): {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`결측 데이터 처리`\n",
    "- 방법1: 결측 데이터 무시 및 삭제\n",
    "- 방법2: 데이터가 없는 부분을 해당 열의 모든 값의 평균으로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(x[:,1:3])\n",
    "x[:,1:3] = imputer.transform(x[:,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 머신러닝 모델은 수치형 데이터를 처리하므로, 범주형 데이터를 수치형으로 변환하여 모델에 입력할 수 있도록 하기 위해 사용됩니다.\n",
    "- 범주형 데이터: 데이터 값이 특정 범주(category) 또는 그룹에 속하는 값을 나타내며, 수치적 연산이 아닌 구분이나 라벨로서 의미를 갖는 데이터\n",
    "- `원핫 인코딩 (One-Hot Encoding)`: 범주형 데이터를 이진 벡터로 변환하여 각 범주를 고유한 열로 표현하고, 값이 해당 범주에 속하면 1, 아니면 0으로 표시하는 인코딩 방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [0.0, 1.0, 0.0, 0.0, 1.0, 27.0, 48000.0],\n",
       "       [0.0, 1.0, 0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [0.0, 1.0, 0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 1.0, 0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [0.0, 1.0, 0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "X= np.array(ct.fit_transform(x))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling을 적용하기 전에 데이터셋을 학슴용과 테스트용으로 분할 하는 이유는 데이터 누수(Data Leakage)를 방지하기 위해서 이다.\n",
    "- 테스트 데이터 정보 유출 방지: Feature Scaling은 데이터 분포를 기반으로 수행되므로, 전체 데이터에 적용하면 테스트 데이터의 통계정보가 학습 과정에 포함될 위험이 있다. 이는 모델 평가의 공정성을 해칠 수 있다.\n",
    "- 실제 환경 시뮬레이션: 테스트 데이터는 훈련되지 않은 새로운 데이터로 간주되어야 하므로, Feature Scaling은 학습 데이터의 통계정보로만 수행해야 평가가 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train(8): [[0.0 1.0 0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 35.0 58000.0]]\n",
      "X_test(2): [[0.0 1.0 0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 37.0 67000.0]]\n",
      "Y_train(8): [0 1 0 0 1 1 0 1]\n",
      "Y_test(2): [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state=1)\n",
    "\n",
    "print(f\"X_train({len(X_train)}): {X_train}\")\n",
    "print(f\"X_test({len(X_test)}): {X_test}\")\n",
    "print(f\"Y_train({len(Y_train)}): {Y_train}\")\n",
    "print(f\"Y_test({len(Y_test)}): {Y_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Feature Scaling`은 머신러닝 모델의 성능을 향상시키기 위해 데이터의 범위를 조정하는 전처리 과정이다.\n",
    "\n",
    "- **Standardisation**: 데이터를 평균을 0, 표준 편차를 1로 조정하여 값을 표준 정규 분포로 변환하는 방법\n",
    "\n",
    "\\[\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "\\]\n",
    "\n",
    "- **Normalisation**: 데이터를 최소값 0, 최대값 1 사이의 범위로 변환하는 방법\n",
    "\n",
    "\\[\n",
    "x' = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 0.0, 1.0, -0.19159184384578545,\n",
       "        -1.0781259408412425],\n",
       "       [0.0, 1.0, 0.0, 1.0, 0.0, -0.014117293757057777,\n",
       "        -0.07013167641635372],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 0.566708506533324, 0.633562432710455],\n",
       "       [0.0, 1.0, 0.0, 0.0, 1.0, -0.30453019390224867,\n",
       "        -0.30786617274297867],\n",
       "       [0.0, 1.0, 0.0, 0.0, 1.0, -1.9018011447007988, -1.420463615551582],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, 1.1475343068237058, 1.232653363453549],\n",
       "       [0.0, 1.0, 0.0, 1.0, 0.0, 1.4379472069688968, 1.5749910381638885],\n",
       "       [1.0, 0.0, 1.0, 0.0, 0.0, -0.7401495441200351,\n",
       "        -0.5646194287757332]], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:,5:] = sc.fit_transform(X_train[:,5:])\n",
    "X_test[:,5:] = sc.transform(X_test[:,5:])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
